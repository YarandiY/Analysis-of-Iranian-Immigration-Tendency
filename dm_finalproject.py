# -*- coding: utf-8 -*-
"""DM_FinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YO8dSx0Ja6kQBlsS87w0zywCWejJn0H1

# **Analysis of Iranian Immigration Tendency**
Final project of Data Mining Course

The dataset that we used is the information of some people and their tendency to immigrating
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
import warnings
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.tree import DecisionTreeClassifier
warnings.filterwarnings('ignore')

from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import linear_model

# Access to google drive
from google.colab import drive
drive.mount('/content/gdrive')

"""# Loading dataset"""

# Read dataset from google drive
df = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/migration.csv')
df.head()

df.shape

"""# Data cleaning

## Select specific columns
"""

df = df.drop(df.iloc[:,:63], axis=1)
df = df.drop(df.iloc[:,9:24], axis=1)
# df.drop(['political'], axis=1, inplace=True)
df.head(5)

df.shape

"""Merge 'education' and 'last_degree' columns:"""

nan_edu = df[df['education'].isnull()]
nan_edu['last_degree'].isnull().sum()

"""There is only 2 rows that have NAN education and last_degree. So we can eliminate these two rows:"""

nan_edu_deg = nan_edu[nan_edu['last_degree'].isnull()]
df = df.drop(nan_edu_deg.index, axis=0)
df.shape

# make sure there is not any row with this condition anymore
nan_edu = df[df['education'].isnull()]
nan_edu['last_degree'].isnull().sum()

df.head(20)

def mix_cols(row):
  if np.isnan(row['education']):
    return row['last_degree']
  return row['education']

df['education'] = df.apply(lambda row: mix_cols(row), axis=1)
df.head(20)

df['education'].isnull().sum()

"""There is no NAN values in education column.

Now we can drop 'last_degree' column:
"""

df = df.drop(columns="last_degree")
df.head()

# Display total number of missing values for each column (sorted from highest to lowest one) after all cleanings
def missing_table(dataset):
  total = dataset.isnull().sum().sort_values(ascending=False)
  percent = (dataset.isnull().sum() / dataset.isnull().count()).sort_values(ascending=False)
  missing_data = pd.concat([total, percent], axis=1, keys=['total', 'percent']) 
  return missing_data.T

missing_table(df)

# remove row with NAN value of label
df = df.drop(df[df['tendency_immigration'].isnull()].index, axis=0)
df = df.drop(df[df['immigration_attitude'].isnull()].index, axis=0)
df.shape

# fill the only row with NAN gender to 'Men'
nan_gen = df[df['gender'].isnull()]
df.loc[nan_gen.index, 'gender'] = 2

missing_table(df)

# One Hot Encoding
df = pd.get_dummies(df, columns=["gender"])
df = df.rename(columns={'gender_1.0':'woman', 'gender_2.0':'man'})
df.head(5)

# To reach binary classification later
df['tendency_immigration'] = df['tendency_immigration'] > 4
df.head()

def changeLabel(value):
  if pd.isnull(value):
    return value
  return str(int(value))

def changeType(column_name):
  df[column_name] = df.apply(lambda row: changeLabel(row[column_name]), axis=1)

# changeType('tendency_immigration')
changeType('education')
changeType('economic')
changeType('religiosity')
changeType('political')
changeType('immigration_attitude')
df.head(5)

category_type = pd.CategoricalDtype(categories=['1', '2', '3', '4', '5', '6', '7'], ordered=True)
# df['tendency_immigration'].astype(category_type)
df['education'].astype(category_type)
df['economic'].astype(category_type)
df['religiosity'].astype(category_type)
df['political'].astype(category_type)
df['immigration_attitude'].astype(category_type)

"""## Create Training and Test Sets"""

# Split-out validation dataset
X = df[list(df.columns)]
X.drop(['tendency_immigration'], axis=1, inplace=True)
y = df['tendency_immigration']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=7)
# describes info about train and test set 
print("X_train dataset: ", X_train.shape) 
print("y_train dataset: ", y_train.shape) 
print("X_test dataset: ", X_test.shape) 
print("y_test dataset: ", y_test.shape)

"""## Handle Missing Values"""

# fill NAN 'age' in test and train sets with mean value of train set
X_train['age'].fillna(np. floor(np.nanmean(X_train['age'])), inplace = True)
X_test['age'].fillna(np.nanmean(X_train['age']), inplace = True)

# check all NAN values filled in 'age' column
print(X_train['age'].isnull().sum())
print(X_test['age'].isnull().sum())

# find possible values for economic column
print(df['economic'].unique())

# fill NAN 'economic' in test and train sets with middle of possible values(3)
X_train['economic'].fillna('3', inplace = True)
X_test['economic'].fillna('3', inplace = True)

# check all NAN values filled in 'economic' column
print(X_train['economic'].isnull().sum())
print(X_test['economic'].isnull().sum())

# find possible values for religiosity column
print(df['religiosity'].unique())

# fill NAN 'religiosity' in test and train sets with middle of possible values(3)
X_train['religiosity'].fillna('3', inplace = True)
X_test['religiosity'].fillna('3', inplace = True)

# check all NAN values filled in 'religiosity' column
print(X_train['religiosity'].isnull().sum())
print(X_test['religiosity'].isnull().sum())

# find possible values for political column
print(df['political'].unique())

# fill NAN 'political' in test and train sets with middle of possible values(3)
X_train['political'].fillna('3', inplace = True)
X_test['political'].fillna('3', inplace = True)

# check all NAN values filled in 'political' column
print(X_train['political'].isnull().sum())
print(X_test['political'].isnull().sum())

X_train['img_deference'].fillna(np.nanmean(X_train['img_deference']), inplace = True)
X_test['img_deference'].fillna(np.nanmean(X_train['img_deference']), inplace = True)

# check all NAN values filled in 'political' column
print(X_train['img_deference'].isnull().sum())
print(X_test['img_deference'].isnull().sum())

X_train['img_heroism'].fillna(np.nanmean(X_train['img_heroism']), inplace = True)
X_test['img_heroism'].fillna(np.nanmean(X_train['img_heroism']), inplace = True)

# check all NAN values filled in 'political' column
print(X_train['img_heroism'].isnull().sum())
print(X_test['img_heroism'].isnull().sum())

X_train['img_family'].fillna(np.nanmean(X_train['img_family']), inplace = True)
X_test['img_family'].fillna(np.nanmean(X_train['img_family']), inplace = True)

# check all NAN values filled in 'political' column
print(X_train['img_family'].isnull().sum())
print(X_test['img_family'].isnull().sum())

X_train['img_fairness'].fillna(np.nanmean(X_train['img_fairness']), inplace = True)
X_test['img_fairness'].fillna(np.nanmean(X_train['img_fairness']), inplace = True)

# check all NAN values filled in 'political' column
print(X_train['img_fairness'].isnull().sum())
print(X_test['img_fairness'].isnull().sum())

X_train['img_property'].fillna(np.nanmean(X_train['img_property']), inplace = True)
X_test['img_property'].fillna(np.nanmean(X_train['img_property']), inplace = True)

# check all NAN values filled in 'political' column
print(X_train['img_property'].isnull().sum())
print(X_test['img_property'].isnull().sum())

X_train['img_Reciprocity'].fillna(np.nanmean(X_train['img_Reciprocity']), inplace = True)
X_test['img_Reciprocity'].fillna(np.nanmean(X_train['img_Reciprocity']), inplace = True)

# check all NAN values filled in 'political' column
print(X_train['img_Reciprocity'].isnull().sum())
print(X_test['img_Reciprocity'].isnull().sum())

missing_table(X_train)

missing_table(X_test)

X_train.head()

"""## Feature Extraction

"""

X_train['age_eco_tend'] = ~((X_train['age'] > 30) & (X_train['economic'] < str(4)))
X_test['age_eco_tend'] = ~((X_test['age'] > 30) & (X_test['economic'] < str(4)))

X_train.head(10)

"""# Plots

"""

plt.title('label balance in train set')
X_train['education'].value_counts().sort_index().plot(kind='bar')
plt.title('education balance')
plt.xlabel('education values')
plt.ylabel('amount per label')
plt.show()

X_train['education'].unique()

plt.title('label balance in train set')
X_train['age'].value_counts().sort_index().plot(kind='bar')
plt.title('age balance')
plt.xlabel('age values')
plt.ylabel('amount per label')
plt.show()

X_train['economic'].unique()

plt.title('label balance in train set')
X_train['economic'].value_counts().sort_index().plot(kind='bar')
plt.title('economic balance')
plt.xlabel('economic values')
plt.ylabel('amount per label')
plt.show()

plt.title('label balance in train set')
X_train['religiosity'].value_counts().sort_index().plot(kind='bar')
plt.title('religiosity balance')
plt.xlabel('religiosity values')
plt.ylabel('amount per label')
plt.show()

plt.title('label balance in train set')
X_train['political'].value_counts().sort_index().plot(kind='bar')
plt.title('political balance')
plt.xlabel('political values')
plt.ylabel('amount per label')
plt.show()
# X_train['political'].unique()

plt.title('label balance in train set')
X_train['immigration_attitude'].value_counts().sort_index().plot(kind='bar')
plt.title('immigration_attitude balance')
plt.xlabel('immigration_attitude values')
plt.ylabel('amount per label')
plt.show()

number_of_men = X_train['man'].sum()
number_of_women = X_train['woman'].sum()
print('number of men : ', number_of_men)
print('number of women : ', number_of_women)

plt.title('label balance in train set')
y_train.value_counts().sort_index().plot(kind='bar')
plt.title('label balance')
plt.xlabel('label values')
plt.ylabel('amount per label')
plt.show()

plt.title('label balance in train set')
y_test.value_counts().sort_index().plot(kind='bar')
plt.title('label balance')
plt.xlabel('label values')
plt.ylabel('amount per label')
plt.show()

plt.plot(X_train['age'], y_train, 'x', color='black')

plt.plot(X_train['education'], y_train, 'x', color='black')

"""# Classifier

## Normalization & Balancing
"""

# apply Smote on train set to get balanced training set
# from imblearn.over_sampling import SMOTE
# from imblearn.under_sampling import RandomUnderSampler
# from imblearn.pipeline import Pipeline
# oversample = SMOTE()
# X_train, y_train = oversample.fit_resample(X_train, y_train)
# print("After Undersampling, counts of label '1': {}".format(sum(y_train == 1))) 
# print("After Undersampling, counts of label '0': {}".format(sum(y_train == 0)))

# apply near miss on train set
from imblearn.under_sampling import NearMiss 
nr = NearMiss() 
X_train, y_train = nr.fit_sample(X_train, y_train.ravel()) 
print("After Undersampling, counts of label '1': {}".format(sum(y_train == 1))) 
print("After Undersampling, counts of label '0': {}".format(sum(y_train == 0)))

# data normalization with sklearn
from sklearn.preprocessing import MinMaxScaler

# fit scaler on training data
norm = MinMaxScaler().fit(X_train)

# transform training data
X_train = norm.transform(X_train)

# transform testing data
X_test = norm.transform(X_test)

"""## Models

"""

def predict(clf):
  predictions = clf.predict(X_test)
  print(classification_report(y_test, predictions)) 
  print(accuracy_score(y_test, predictions))

"""### Decision Tree"""

clf = DecisionTreeClassifier(class_weight='balanced')
clf.fit(X_train, y_train)
predict(clf)

"""### Logistic Regression"""

logistic_regression = Pipeline([('clf', linear_model.LogisticRegression(n_jobs=1, C=1e5))]);
model = logistic_regression.fit(X_train, y_train)
predict(model)

"""### KNN"""

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=11)
classifier.fit(X_train, y_train)
predict(classifier)

"""### SVM"""

from sklearn.svm import SVC
svclassifier = SVC(kernel='linear')
svclassifier.fit(X_train, y_train)
predict(svclassifier)

"""### Random Forest

Using Grid search to find best parameters:
"""

# # Number of trees in random forest
# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
# # Number of features to consider at every split
# max_features = ['auto', 'sqrt']
# # Maximum number of levels in tree
# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
# max_depth.append(None)
# # Minimum number of samples required to split a node
# min_samples_split = [2, 5, 10]
# # Minimum number of samples required at each leaf node
# min_samples_leaf = [1, 2, 4]
# # Method of selecting samples for training each tree
# bootstrap = [True, False]
# # Create the random grid
# random_grid = {'n_estimators': n_estimators,
#                'max_features': max_features,
#                'max_depth': max_depth,
#                'min_samples_split': min_samples_split,
#                'min_samples_leaf': min_samples_leaf,
#                'bootstrap': bootstrap}

# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import RandomizedSearchCV
# rfc = RandomForestClassifier(class_weight='balanced')
# rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)
# # Fit the random search model
# rf_random.fit(X_train, y_train)

# rf_random.best_params_

# best_random = rf_random.best_estimator_
# predictions = best_random.predict(X_test)
# print(classification_report(y_test, predictions)) 
# print(accuracy_score(y_test, predictions))

"""Now using best parameters calculated from grid search:"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
rfc = RandomForestClassifier(bootstrap= True, 
                            max_depth= 30,
                            max_features= 'sqrt',
                            min_samples_leaf= 1,
                            min_samples_split= 5,
                            n_estimators= 400, class_weight='balanced')
rfc.fit(X_train, y_train)
predict(rfc)

"""### NB"""

model3 =  MultinomialNB().fit(X_train, y_train)
predict(model3)

"""# Cluster"""

# Try to cluster data with k-means approach... Just for fun:)
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=4, init='k-means++', max_iter=300, n_init=10, random_state=0)
pred_y = kmeans.fit_predict(X_train)
plt.scatter(X_train[:,0], X_train[:,1])
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')
plt.show()

df2 = df[list(df.columns)]
df2.drop(['woman', 'man','img_family','img_group','img_Reciprocity','img_heroism','img_deference','img_fairness','img_property', 'mac_family',	'mac_group',	'mac_reciprocity',	'mac_heroism',	'mac_deference',	'mac_fairness',	'mac_property'], axis=1, inplace=True)
df2.head()

#Importing required modules
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import numpy as np
df2 = df2.fillna(0)
print(df2.columns)
data = df2
pca = PCA(2)
#Transform the data
df2 = pca.fit_transform(data)
df2.shape

#Import required module
from sklearn.cluster import KMeans
 
#Initialize the class object
kmeans = KMeans()
 
#predict the labels of clusters.
label = kmeans.fit_predict(df2)
 
print(np.unique(label))

